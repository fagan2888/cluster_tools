#!/usr/bin/env python3
import logging
logging.basicConfig(format='%(message)s')
import os.path
import json
import click
import uuid
import invoke
from getpass import getpass
from fabric import Connection
from cluster_tools.pack import pack
from time import sleep
from colorama import Style, Fore
import tarfile
from types import SimpleNamespace as Namespace

def untar(conn, tarball, path):
  """ untar the remote file to the path
  """
  conn.run(f"tar xf {tarball} -C {path}")


def sbatch_script(conn, options, commands, sbatch_script_name, verbose=True):
  options.update({
      "--output": "slurm.out",
      "--error": "slurm.err",
  })
  def print_if_verbose(*args, **kwargs):
    if verbose:
      logging.info(*args, **kwargs)
  with open(sbatch_script_name, "w") as f:
    f.write("#!/bin/bash\n")
    for k, v in options.items():
      f.write(f"#SBATCH {k} {v}\n")
    for command in commands:
      f.write(f"{command}\n")
  with open(sbatch_script_name, "r") as f:
    print_if_verbose(f"{Fore.CYAN}==== SBATCH SCRIPT ====\n")
    print_if_verbose(f.read() + f"{Style.RESET_ALL}")


def sbatch(conn):
  """ run sbatch with options and submit the command
  """
  sbatch_script_name = "./sbatch_script.sh"
  job_id = conn.run(f"sbatch {sbatch_script_name}").stdout.rsplit(None, 1)[-1]
  def check_state(prev_state):
    try:
      state, reason = conn.run(
          f"squeue -j {job_id} -O state,reason -h", hide=True).stdout.split()
    except:
      state = conn.run(f"sacct -j {job_id} --format=state | head -1",
                       hide=True).stdout.strip()
      reason = None
    if state == "PENDING":
      if prev_state != state:
        logging.info(f"{Fore.CYAN}PENDING({reason}){Style.RESET_ALL}")
    elif state == "RUNNING":
      if prev_state != state:
        logging.info(f"{Fore.CYAN}RUNNING{Style.RESET_ALL}")
    elif state == "FAILED":
      if prev_state != state:
        logging.info(f"{Fore.RED}FAILED({reason}){Style.RESET_ALL}")
    else:
      if prev_state != state:
        logging.info(f"{Fore.RED}{state}({reason}){Style.RESET_ALL}")
    return state

  state = None
  while True:
    state = check_state(prev_state=state)
    if state == "RUNNING":
      node = conn.run(f"squeue -j {job_id} -O nodelist -h",
                      hide=True).stdout.strip()
      return node, job_id
    elif state == "FAILED":
      stdout = conn.run("cat slurm.out").stdout
      stderr = conn.run("cat slurm.err").stdout
      logging.info(f"{Fore.RED}stdout{Style.RESET_ALL}")
      logging.info(stdout)
      logging.info(f"{Fore.RED}stderr{Style.RESET_ALL}")
      logging.info(stderr)
      return None, job_id
    elif state == "PENDING":
      sleep(10)
      continue
    else:
      logging.info(state)
      return None, job_id


@click.command(context_settings=dict(
    ignore_unknown_options=True,
))
@click.option('-c', type=click.Path(exists=True),
              required=True, help="submit config to use")
@click.option('--sbatch_json', type=str, default="{}",
              help="extra json string to pass sbatch arguments")
@click.option('--tensorboard', is_flag=True, default=True)
@click.argument('command', nargs=-1, type=click.UNPROCESSED, required=True)
def submit(c, sbatch_json, tensorboard, command):
  with open(click.format_filename(c), "r") as f:
    config = json.load(f, object_hook=lambda d: Namespace(**d))

  # create a connection to the remote machine.
  # from which the job will be submitted.
  password = getpass(f"Password for {config.hostname}: ")
  if password:
    conn = Connection(config.hostname, user=config.user, connect_kwargs={"password": password})
  elif hasattr(config, "ssh_key"):
    conn = Connection(config.hostname, user=config.user, connect_kwargs={"key_filename": config.ssh_key})
  else:
    conn = Connection(config.hostname, user=config.user)
  if conn.is_connected:
    print(f"Connection to {Fore.CYAN}{config.hostname}{Style.RESET_ALL} established")

  # pack a tar ball for the current working directory
  # create a dirty branch for the current working tree
  # if the repo is not clean
  tarball, relative_dir = pack()
  assert tarball.endswith(".tar")
  prefix = tarball[:-4]
  logging.info(f"Tarball {Fore.CYAN}{tarball}{Style.RESET_ALL} created for the current repo")

  # prepare sbatch script and add to the tarball
  # TODO: in the future keep track of the sbatch commands with git
  options = {}
  options.update(config.sbatch_options.__dict__)
  if os.path.isfile("./sbatch.json"):
    with open("./sbatch.json") as f:
      extra_options = json.load(f)
      options.update(extra_options)
  # Load extra sbatch options from command line
  extra_options = json.loads(sbatch_json)
  options.update(extra_options)
  # prepare the commands
  command = " ".join(command)
  setup_python = [
      config.activate_python,
  ]
  install_dependency = [
      "pip install -r requirements.txt",
  ]
  tensorboard_cmd = [
      """PORT=$(python -c 'import socket; s=socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(("localhost",0)); print(s.getsockname()[1])')""",
      "echo $PORT > tensorboard.port",
      "tensorboard --logdir ./ --port $PORT &",
  ]
  commands = [
      *setup_python,
      *install_dependency,
      *tensorboard_cmd,
      command,
  ]
  # create sbatch file
  sbatch_script_name = f"/tmp/{uuid.uuid4().hex}"
  sbatch_script(conn, options, commands, sbatch_script_name, verbose=True)
  archive = tarfile.open(f"{tarball}", "a")
  archive.add(sbatch_script_name, arcname=f"{prefix}/{relative_dir}/sbatch_script.sh")
  archive.close()

  # copy the tarball created to the destination directory
  # on the remote machine
  logging.info(f"Uploading {Fore.CYAN}{tarball}{Style.RESET_ALL} to {Fore.CYAN}{config.hostname}:{config.tar_path}/{tarball}{Style.RESET_ALL}")
  try:
    conn.put(tarball, f"{config.tar_path}/{tarball}")
    logging.info("Upload SUCCESSFUL")
  except:
    logging.exception("Upload FAILED")

  # untar the tarball on the remote machine to the
  # specified directory
  try:
    untar(conn, f"{config.tar_path}/{tarball}", config.untar_path)
    logging.info(f"Extracted to {Fore.CYAN}{config.untar_path}/{prefix}{Style.RESET_ALL}")
  except:
    logging.exception("Extraction FAILED")

  # go to the untared dir
  # and execute the command
  with conn.cd(f"{config.untar_path}/{prefix}"):
    with conn.prefix("source $HOME/.bashrc"):
      # launch the sbatch command
      node, job_id = sbatch(conn)
      if not node:
        return
      # launch tensorboard
      while True:
        try:
          port = conn.run("cat tensorboard.port", hide=True).stdout.strip()
          break
        except:
          sleep(5)

      logging.info(f"Tensorboard at port {port}")
      conn.close()
      ssh_key_option = ""
      if hasattr(config, "ssh_key"):
        ssh_key_option = f"-i {config.ssh_key}"
      port_forwarding = f"ssh -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' {ssh_key_option} -J {config.user}@{config.hostname} -N -f -L localhost:{port}:localhost:{port} {config.user}@{node}"
      logging.info(f"Running port forwarding script {Fore.CYAN}{port_forwarding}{Style.RESET_ALL}")
      logging.info(f"Link to tensorboard {Fore.CYAN}http://localhost:{port}{Style.RESET_ALL}")
      logging.info(f"{Fore.CYAN}Ctrl-C to terminate port forwarding{Style.RESET_ALL}")
      context = invoke.Context()
      context.run(port_forwarding)


if __name__ == "__main__":
  submit()
