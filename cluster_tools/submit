#!/usr/bin/env python3
import logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
import os
import os.path
import json
import click
import uuid
import invoke
from fabric import Connection
from invoke import Context, Responder
from cluster_tools.pack import pack
from time import sleep
from colorama import Style, Fore
import tarfile
from types import SimpleNamespace as Namespace
from cluster_tools.connection import connect_entry_node, connect_node

def untar(conn, tarball, path):
  """ untar the remote file to the path
  """
  conn.run(f"tar xf {tarball} -C {path}")


def sbatch_script(conn, options, commands, sbatch_script_name, verbose=True):
  options.update({
      "--output": "slurm.out",
      "--error": "slurm.err",
  })
  def print_if_verbose(*args, **kwargs):
    if verbose:
      logging.info(*args, **kwargs)
  with open(sbatch_script_name, "w") as f:
    f.write("#!/bin/bash\n")
    for k, v in options.items():
      f.write(f"#SBATCH {k} {v}\n")
    for command in commands:
      f.write(f"{command}\n")
    f.write("wait\n")
  with open(sbatch_script_name, "r") as f:
    print_if_verbose(f"{Fore.CYAN}==== SBATCH SCRIPT ====\n")
    print_if_verbose(f.read() + f"{Style.RESET_ALL}")


def sbatch(conn):
  """ run sbatch with options and submit the command
  """
  sbatch_script_name = "./sbatch_script.sh"
  job_id = conn.run(f"sbatch {sbatch_script_name}").stdout.rsplit(None, 1)[-1]
  def check_state(prev_state):
    try:
      state, reason = conn.run(
          f"squeue -j {job_id} -O state,reason -h", hide=True).stdout.split()
    except:
      state = conn.run(f"sacct -j {job_id} --format=state | head -1",
                       hide=True).stdout.strip()
      reason = None
    if state == "PENDING":
      if prev_state != state:
        logging.info(f"{Fore.CYAN}PENDING({reason}){Style.RESET_ALL}")
    elif state == "RUNNING":
      if prev_state != state:
        logging.info(f"{Fore.CYAN}RUNNING{Style.RESET_ALL}")
    elif state == "FAILED":
      if prev_state != state:
        logging.info(f"{Fore.RED}FAILED({reason}){Style.RESET_ALL}")
    else:
      if prev_state != state:
        logging.info(f"{Fore.RED}{state}({reason}){Style.RESET_ALL}")
    return state

  state = None
  while True:
    state = check_state(prev_state=state)
    if state == "RUNNING":
      node = conn.run(f"squeue -j {job_id} -O nodelist -h",
                      hide=True).stdout.strip()
      return node, job_id
    elif state == "FAILED":
      stdout = conn.run("cat slurm.out").stdout
      stderr = conn.run("cat slurm.err").stdout
      logging.info(f"{Fore.RED}stdout{Style.RESET_ALL}")
      logging.info(stdout)
      logging.info(f"{Fore.RED}stderr{Style.RESET_ALL}")
      logging.info(stderr)
      return None, job_id
    elif state == "PENDING":
      sleep(10)
      continue
    else:
      logging.info(state)
      return None, job_id


@click.command(context_settings=dict(
    ignore_unknown_options=True,
))
@click.option('-c', type=click.Path(exists=True),
              required=True, help="submit config to use")
@click.option('--name', '-n', type=str, default="default", help="name of the project")
@click.option('--branch', '-b', type=str, default="", help="branch name")
@click.option('--sbatch_json', type=str, default="{}",
              help="extra json string to pass sbatch arguments")
@click.option('--tensorboard', is_flag=True, default=True)
@click.argument('command', nargs=-1, type=click.UNPROCESSED, required=True)
def submit(c, name, branch, sbatch_json, tensorboard, command):
  with open(click.format_filename(c), "r") as f:
    config = json.load(f, object_hook=lambda d: Namespace(**d))

  # create a connection to the remote machine.
  # from which the job will be submitted.
  conn = connect_entry_node(config)

  # pack a tar ball for the current working directory
  # create a dirty branch for the current working tree
  # if the repo is not clean
  tarball, prefix, exec_path = pack(name, branch)
  assert tarball.endswith(".tar")
  logging.info(f"Tarball {Fore.CYAN}{tarball}{Style.RESET_ALL} created for the current repo")

  # prepare sbatch script and add to the tarball
  # TODO: in the future keep track of the sbatch commands with git
  options = {}
  options.update(config.sbatch_options.__dict__)
  if os.path.isfile("./sbatch.json"):
    with open("./sbatch.json") as f:
      extra_options = json.load(f)
      options.update(extra_options)
  # Load extra sbatch options from command line
  extra_options = json.loads(sbatch_json)
  options.update(extra_options)
  # prepare the commands
  command = " ".join(command)
  setup_python = [
      config.activate_python,
  ]
  relative_dir = "../" * len(list(filter(None, exec_path.split("/"))))
  install_dependency = [
      f"pip install -r {relative_dir}requirements.txt --upgrade",
  ]
  tensorboard_cmd = [
      """PORT=$(python -c 'import socket; s=socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(("localhost",0)); print(s.getsockname()[1])')""",
      "echo $SLURM_JOB_ID > SLURM_JOB_ID",
      "echo $PORT > tensorboard.port",
      "tensorboard --logdir ./ --port $PORT &",
  ]
  commands = [
      *setup_python,
      *install_dependency,
      *tensorboard_cmd,
      command,
  ]
  # create sbatch file
  sbatch_script_name = f"/tmp/{uuid.uuid4().hex}"
  sbatch_script(conn, options, commands, sbatch_script_name, verbose=True)
  archive = tarfile.open(f"{tarball}", "a")
  archive.add(sbatch_script_name, arcname=f"{prefix}/{exec_path}/sbatch_script.sh")
  archive.close()

  # copy the tarball created to the destination directory
  # on the remote machine
  logging.info(f"Uploading {Fore.CYAN}{tarball}{Style.RESET_ALL} to {Fore.CYAN}{config.hostname}:{config.tar_path}/{tarball}{Style.RESET_ALL}")
  try:
    conn.put(tarball, f"{config.tar_path}/{tarball}")
    logging.info("Upload SUCCESSFUL")
  except:
    logging.exception("Upload FAILED")

  # untar the tarball on the remote machine to the
  # specified directory
  try:
    untar(conn, f"{config.tar_path}/{tarball}", config.untar_path)
    logging.info(f"Extracted to {Fore.CYAN}{config.untar_path}/{prefix}{Style.RESET_ALL}")
  except:
    logging.exception("Extraction FAILED")

  # go to the untared dir
  # and execute the command
  with conn.cd(f"{config.untar_path}/{prefix}/{exec_path}"):
    with conn.prefix("source $HOME/.bashrc"):
      # launch the sbatch command
      node, job_id = sbatch(conn)
      if not node:
        return
      logging.info(f"Job submitted to node {node}")
      # launch tensorboard
      while True:
        try:
          port = conn.run("cat tensorboard.port", hide=True).stdout.strip()
          break
        except:
          sleep(5)

      context = Context()
      logging.info(f"Tensorboard at port {port}")
      while True:
        try:
          logging.info(f"forwarding local port to {node}")
          logging.info(f"Link to tensorboard {Fore.CYAN}http://localhost:{port}{Style.RESET_ALL}")
          logging.info(f"{Fore.CYAN}Ctrl-C to terminate port forwarding{Style.RESET_ALL}")
          logging.info("Now forwarding the job output to terminal")
          r = Responder(pattern=r".* password:", response=(conn.connect_kwargs.get("password", "") + "\n"))
          context.run(f"ssh -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' -J {conn.user}@{conn.host} -L localhost:{port}:localhost:{port} {conn.user}@{node}", watchers=[r], pty=True)

        except Exception as e:
          logging.info(e)

        logging.info("\nDisconnecting from remote tensorboard")
        if click.confirm(f"Do you want to abort the job {job_id}?", default=False):
          conn.close()
          conn.open()
          conn.run(f"scancel {job_id}")
          conn.close()
          break
        else:
          logging.info("Reconnecting ... ")
          continue


if __name__ == "__main__":
  submit()
